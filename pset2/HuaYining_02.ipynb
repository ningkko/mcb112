{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8311ba",
   "metadata": {},
   "source": [
    "# Problem set 2\n",
    "### Ning Hua\n",
    "\n",
    "## Q1 - use kallisto and reproduce Moriarty's result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749d80eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kallisto 0.46.2\r\n",
      "\r\n",
      "Usage: kallisto <CMD> [arguments] ..\r\n",
      "\r\n",
      "Where <CMD> can be one of:\r\n",
      "\r\n",
      "    index         Builds a kallisto index \r\n",
      "    quant         Runs the quantification algorithm \r\n",
      "    bus           Generate BUS files for single-cell data \r\n",
      "    pseudo        Runs the pseudoalignment step \r\n",
      "    merge         Merges several batch runs \r\n",
      "    h5dump        Converts HDF5-formatted results to plaintext\r\n",
      "    inspect       Inspects and gives information about an index\r\n",
      "    version       Prints version information\r\n",
      "    cite          Prints citation information\r\n",
      "\r\n",
      "Running kallisto <CMD> without arguments prints usage information for <CMD>\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!kallisto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe98be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[build] loading fasta file arc.fasta.gz\r\n",
      "[build] k-mer length: 31\r\n",
      "[build] counting k-mers ... done.\r\n",
      "[build] building target de Bruijn graph ...  done \r\n",
      "[build] creating equivalence classes ...  done\r\n",
      "[build] target de Bruijn graph has 19 contigs and contains 10000 k-mers \r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!kallisto index -i arc.idx arc.fasta.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d49e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[quant] fragment length distribution is truncated gaussian with mean = 150, sd = 20\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 10\n",
      "[index] number of k-mers: 10,000\n",
      "[index] number of equivalence classes: 26\n",
      "[quant] running in single-end mode\n",
      "[quant] will process file 1: arc.fastq.gz\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 100,000 reads, 99,980 reads pseudoaligned\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 62 rounds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! kallisto quant -i arc.idx -o arc_output --single -l 150 -s 20 arc.fastq.gz                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb636c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abundance.h5  abundance.tsv run_info.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls arc_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747fc45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_id\tlength\teff_length\test_counts\ttpm\r\n",
      "Arc6\t3000\t2851\t1729.89\t17073.6\r\n",
      "Arc1\t4000\t3851\t2608.95\t19063.2\r\n",
      "Arc9\t3000\t2851\t2568.63\t25351.7\r\n",
      "Arc2\t2000\t1851\t3626.07\t55123\r\n",
      "Arc4\t4000\t3851\t10613.7\t77552.6\r\n",
      "Arc7\t2000\t1851\t5497.17\t83567.2\r\n",
      "Arc8\t2000\t1851\t5785.68\t87953.1\r\n",
      "Arc5\t4000\t3851\t12635.9\t92328.7\r\n",
      "Arc10\t3000\t2851\t26310.9\t259682\r\n",
      "Arc3\t3000\t2851\t28603.1\t282305\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"^#\" arc_output/abundance.tsv\n",
    "!grep -v \"^#\" arc_output/abundance.tsv| sort -n -k5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064c7123",
   "metadata": {},
   "source": [
    "So the TPM results given by Kallisto is way closer to what Moriarty reported with a simple glimpse. And Arc6 and Arc1 are indeed more similar to Arc9 (talking about TPM, see table above). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c075a1e9",
   "metadata": {},
   "source": [
    "## Q2\n",
    "### b. simulate an Arc transcriptome and RNA-seq reads - alternative (harder) version of part 2\n",
    "Write Python code to simulate an Arc locus, an Arc transcriptome, and 100,000 reads from an RNA-seq experiment on the Arc transcriptome. Your script generates two simulated data files that kallisto will take as its input:\n",
    "\n",
    "a FASTA format file of the transcriptome\n",
    "a FASTQ format file of the reads\n",
    "Your simulation will be controlled by several parameters. It'll be useful to leave them as settable parameters that you can play with in different simulations. For example, here's a chunk from mine:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b9a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Arc locus \n",
    "## I made some changes to the names for easier reference\n",
    "n_transcripts = 10                        # Number of segments in the Arc locus (A..J)\n",
    "n_reads = 100000                          # total number of observed reads we generate\n",
    "seg_len = 1000                            # length of each segment (nucleotides)\n",
    "arc_len = seg_len * n_transcripts         # total length of the Arc locus (nucleotides)\n",
    "read_len = 75                             # read length\n",
    "basic_phases = \"A C G T\".split()\n",
    "transcript_lengths = [4000, 2000, 3000, 4000, 4000, \n",
    "                      3000, 2000, 2000, 3000, 3000]  ## copied from the pset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12980c",
   "metadata": {},
   "source": [
    "Set up your simulation so that it can either use the structure of the Arc locus shown above (i.e. those specific lengths Li and those specific abundances νi), or it can generate new random choices for lengths of each transcript (in segments) Li and abundances νi.\n",
    "\n",
    "To generate an Arc locus DNA sequence:\n",
    "- assume S=10 segments, A-J\n",
    "- assume each segment is 1000 bp long\n",
    "- make random DNA of uniform base composition, 25% each base (ACGT)\n",
    "- your total DNA sequence will be 10,000 bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9905d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "rand.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85fe580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_arc_loc(arc_len: int=arc_len):\n",
    "    \"\"\"Generates a DNA seqeunce, which is a list\"\"\"\n",
    "    arc_locus = []\n",
    "    ## random generation from base phases\n",
    "    for i in range(arc_len):\n",
    "        arc_locus.append(np.random.choice(basic_phases, p=[0.25,0.25,0.25,0.25]))\n",
    "    \n",
    "    return \"\".join(arc_locus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c61b8a",
   "metadata": {},
   "source": [
    "To generate the Arc1...Arc10 mRNA transcripts:\n",
    "- extract them from the Arc locus by their coordinates (being careful to handle the circular permutation!)\n",
    "- write them in FASTA format to a .fasta output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5a79228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arc_to_transcriptome(arc_locus:str, transcript_lengths:list=transcript_lengths):\n",
    "    \"\"\"\n",
    "    Given an arc lotus, returns a transcriptome\n",
    "    By defult this function uses transcript_lengths provided in the pset\n",
    "    \"\"\"\n",
    "    arc_transcripts = []\n",
    "    seg_start = 0   ## tracking the start position of each transcript\n",
    "    \n",
    "    i=0\n",
    "    for transcript_len in transcript_lengths:\n",
    "        first_seg = arc_locus[seg_start:]  ## there might be a second part in a new circle \n",
    "        if seg_start + transcript_len > len(arc_locus):  ## if completes a circle\n",
    "            arc_transcripts.append(first_seg + arc_locus[:(transcript_len-len(first_seg))])  ## find the second part in the new circle\n",
    "        else:\n",
    "            arc_transcripts.append(arc_locus[seg_start:seg_start+transcript_len])\n",
    "        seg_start += seg_len\n",
    "        if seg_start >= len(arc_locus):\n",
    "            seg_start = 0\n",
    "\n",
    "    return arc_transcripts\n",
    "\n",
    "def find_num_lines(str_len, line_max_char):\n",
    "    \"\"\"\n",
    "    returns the number of lines to fit a string given the max number of chars a line\n",
    "    \"\"\"\n",
    "    if str_len%line_max_char == 0:\n",
    "        return int(str_len/line_max_char)\n",
    "    else:\n",
    "        return int(str_len/line_max_char)+1\n",
    "        \n",
    "def write_transcriptome(arc_transcripts, output_file):\n",
    "    \"\"\"\n",
    "    writes our transcripts to a fasta file \n",
    "    \"\"\"\n",
    "    line_max_char = 80\n",
    "    with open(output_file, \"w\") as file:\n",
    "        for transcript_num, transcript in enumerate(arc_transcripts):\n",
    "            file.write(\">\"+\"Arc\"+str(transcript_num + 1)+\"\\n\")\n",
    "            ## split them to lines of 80 characters\n",
    "            trans_l = [transcript[i*line_max_char:(i+1)*line_max_char] \n",
    "                       for i in range(find_num_lines(len(transcript), line_max_char))]  \n",
    "            file.write(\"\\n\".join(trans_l)+\"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4693bb",
   "metadata": {},
   "source": [
    "To generate reads, kallisto models the cDNA library fragment length distribution (so that it can calculate an \"effective length\" of each mRNA, correcting for the fact that library fragmentation and size selection selects against small cDNAs). So to generate each read, first have your simulation generate a random fragment, then generate a read from one of its ends:\n",
    "- sample a transcript i according to its nucleotide abundance νi;\n",
    "- pick a random fragment length at least as long as one read from a truncated Gaussian of mean length 150, standard deviation 20 (truncated because: no shorter than a read, and no longer than the whole transcript), i.e. something like:\n",
    "```\n",
    "while True:\n",
    "    fraglen = int(np.random.normal(mean_frag, sd_frag))\n",
    "    if fraglen >= len_R: break\n",
    "if fraglen > L[i]: fraglen = L[i]\n",
    "```\n",
    "- pick a random fragment of that length from transcript i;\n",
    "- pick a 75nt read by choosing a random orientation, taking the read from one of the two ends of the fragment. If on the top strand, your read is the first 75nt of the fragment. If on the bottom strand, your read is the last 75nt of the fragment, reverse complemented.\n",
    "- generate a simulated read sequence by adding simulated base calling errors to the 75nt read: given base calling accuracy α, at each base, with probability (1−α), change it to one of the other 3 bases.\n",
    "- output the 75nt read starting from that position in FASTQ format to your read file. Repeat for all 100K reads.\n",
    "\n",
    "In this version, in your kallisto quant step, you'll give kallisto the same fragment length distribution parameters you used to simulate the data, i.e.:\n",
    "```\n",
    "kallisto quant -i <indexfile> -o <outputdir> --single -l 150 -s 20 <fastqfile>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da303a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_abundance = [0.008, 0.039, 0.291, 0.112, 0.127, \n",
    "                     0.008, 0.059, 0.060, 0.022, 0.273] ## copied from the pset            \n",
    "\n",
    "gaussian_mean = 150\n",
    "gaussian_sd = 20\n",
    "alpha = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "870ed12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GCCCTAA'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_d = {\"A\":\"T\", \"T\":\"A\", \"C\":\"G\", \"G\":\"C\"}   ## complimentary \n",
    "def find_reverse_comp(seq):\n",
    "    return \"\".join(list(map(lambda x: comp_d.get(x), seq[::-1])))  \n",
    "\n",
    "find_reverse_comp(\"TTAGGGC\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "600950a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CCATACGCGTATTCGTGAAT'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mutate_with_base_error(seq:str, alpha=alpha):\n",
    "    \"\"\"mutates a sequence with a given base error alpha\"\"\"\n",
    "    def mutate(c):\n",
    "        return np.random.choice([bp for bp in basic_phases if bp!=c]) if rand.uniform(0,1) <= (1-alpha) else c\n",
    "    return \"\".join(list(map(lambda x: mutate(x), seq)))\n",
    "mutate_with_base_error(\"AAACCCGGGTTTTATATATA\", 0.5)  ## test, if alpha=0.5, about half of the bases should mutate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d72a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(l):\n",
    "    \"\"\"\n",
    "    normalizes a list of floats\n",
    "    \"\"\"\n",
    "    return [i/sum(l) for i in l]\n",
    "\n",
    "def gaussian_read_len(sample_max_len, sample_min_len=read_len, mean=gaussian_mean, sd=gaussian_sd):\n",
    "    \"\"\"\n",
    "    pick a random fragment length at least as long as one read from a truncated Gaussian of mean length 150, \n",
    "    standard deviation 20 (truncated because: no shorter than a read, and no longer than the whole transcript)\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        fraglen = int(np.random.normal(mean, sd))\n",
    "        if fraglen >= sample_min_len: break\n",
    "    if fraglen > sample_max_len: fraglen = sample_max_len\n",
    "    return fraglen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce3164d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apparently my notebook is not configured properly\n",
    "import sys\n",
    "sys.path =  ['/opt/homebrew/anaconda3/envs/mcb/lib/python39.zip',\n",
    "             '/opt/homebrew/anaconda3/envs/mcb/lib/python3.9',\n",
    "             '/opt/homebrew/anaconda3/envs/mcb/lib/python3.9/lib-dynload',\n",
    "             '/opt/homebrew/anaconda3/envs/mcb/lib/python3.9/site-packages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e037d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def transcripts_to_reads(transcripts:list, abundance:list=default_abundance):\n",
    "    reads = []\n",
    "    for i in trange(n_reads):\n",
    "        ## sample based on aundance\n",
    "        transcript_idx = np.random.choice(len(transcripts), p=normalize(abundance))\n",
    "        transcript = transcripts[transcript_idx]  ## @param: str\n",
    "        transcript_len = len(transcript)\n",
    "        ## random read length that's shorter than the transcript length but longer than the default read_len\n",
    "        frag_len = gaussian_read_len(sample_max_len=transcript_len, sample_min_len=read_len)\n",
    "        start_idx = np.random.randint(0, (transcript_len-frag_len)+1)\n",
    "        frag = transcript[start_idx:start_idx+frag_len]\n",
    "        ## reverse complement w/ 1/2 probability\n",
    "        if np.random.uniform(0,1) > 0.5:\n",
    "            reads.append(mutate_with_base_error(frag[:75]))\n",
    "        else:\n",
    "            reads.append(mutate_with_base_error(find_reverse_comp(frag)))\n",
    "    return reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45bd4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_reads(reads, output_file):\n",
    "    with open(output_file, \"w\") as file:\n",
    "        for i, read in enumerate(reads):\n",
    "            file.write(\"@read%s\\n\"%str(i))\n",
    "            file.write(read+\"\\n\")\n",
    "            file.write(\"+\\n\")\n",
    "            file.write(\"I\"*len(read)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22ead6",
   "metadata": {},
   "source": [
    "## 3. Test kallisto\n",
    "Use your simulator (from 2a or 2b above) to generate dataset(s) using the lengths for the Arc locus as shown above, and the abundances that you think are true (first table, above). Analyze the simulated data with kallisto.\n",
    "\n",
    "Does kallisto get the correct answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6e5dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "locus_simulation = generate_arc_loc(arc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0867a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Arc1\r\n",
      "GCATGCAAAGGTACATCAGATCCTGCGGTTGGGTGCCAACCCAAGTGTGTTCACGGGCGCTTGACAGACATCGGAGGATG\r\n",
      "GTGCACACTCACTCGACCAGCGCAAAGCACAGGATCTCACGGGCGGACATCTCTTAGGTCAGTCATCGTGGAGGAATGCT\r\n",
      "TGTACGTTCTTTTGGCTTCCCCTAACACGGCGGGCGTCTCCGGTACGTATCCTGTCGGTACACCCCTTAAGCCCCTAGGC\r\n",
      "CCGAAGAACATAGCGCATTTCACGCTCTCTACGAATGACCGCAACGATCAAATGGGCGAGAACAACTAATTCCGATTCAT\r\n",
      "GGGGTTTGTGGATTGTGACACAGCGCGCCCGCTACTGCGGGACGTGAGGACGCCCAATTCTGCCAAGGATTATTTAGGGT\r\n",
      "GTTTCACTAGAGTTATGCGCCGACCCCGGTTGGACCAGCTTGCATTCGAAACTGCGTTACACAGCACCCCACCGCAATCG\r\n",
      "TATGACTCTCGCTGAAAAAGGGTGGTCAACCATTACACCTCTTATGCCTGTTGTGGGAGGCTCGGTCTTAAGCAGCGCGC\r\n",
      "GAGCTGTGATCCAGGCTACCACGGACATAGTGTATGGAAAGTGATCCAGAGTAGACCCGCGGGGGCCTGACCTAACCTAT\r\n",
      "ATAAGTTGTATCGTGGCTATGAGGGTAGTCGCCGGAGAAAACGTATGCTTACTGATTTTTAAGTCGGCGTGGCGCCGAAG\r\n"
     ]
    }
   ],
   "source": [
    "transcriptome_simulation = arc_to_transcriptome(locus_simulation)\n",
    "write_transcriptome(transcriptome_simulation, \"arc_simulation.fasta\")\n",
    "!head -10 arc_simulation.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e30992a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:14<00:00, 6767.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AGTCTGAGCAGTTCTTGATAGAGCCCAGTTAGGCAGTGCGATGATCTCACCAGGGAGAGATGGCACTTAGGGAAC',\n",
       " 'GCTTAAGGTCTAGCAAGATCCTTAGGCCTATGAGGAAGGTTGTGAGTTTTAAATCCAGGGGTATAACCCCTACTACCACTGCTGCAATGTCTGCATACCTCGTCCGTCTTCCAGGTTTTTCTTACGTCGAACACTAAACCGCCACGCATGACACT',\n",
       " 'TCCTACCCCACATCCTGATAATGCAGCTATGTGGCAATTCACGCTTACCCAATCCTTAGCTGGCTAACAATTCCC',\n",
       " 'AGTTCTGGCGTCTGAACGTATTATTGTTGCTGGCTATCACAGTTAATTCCCTGCCTACGAATTTTGTCGTACCAA',\n",
       " 'GGAATGATTCGTTGGTCGTTTTTTGTTTTCACGGTCCCGATGCTCCTCTATAGGATAATTAAGAGCAGATTAATGTCTAATATGTGAGCACTAGCGGCTCAGCGCACCAGGAAGCATTCTGAGTACACCCCCCCTCGGCGGATCCCAGAACTTTCGTGCCACA']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reads_simulation = transcripts_to_reads(transcripts=transcriptome_simulation)\n",
    "reads_simulation[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ad65a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@read0\r\n",
      "AGTCTGAGCAGTTCTTGATAGAGCCCAGTTAGGCAGTGCGATGATCTCACCAGGGAGAGATGGCACTTAGGGAAC\r\n",
      "+\r\n",
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\r\n",
      "@read1\r\n",
      "GCTTAAGGTCTAGCAAGATCCTTAGGCCTATGAGGAAGGTTGTGAGTTTTAAATCCAGGGGTATAACCCCTACTACCACTGCTGCAATGTCTGCATACCTCGTCCGTCTTCCAGGTTTTTCTTACGTCGAACACTAAACCGCCACGCATGACACT\r\n",
      "+\r\n",
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\r\n",
      "@read2\r\n",
      "TCCTACCCCACATCCTGATAATGCAGCTATGTGGCAATTCACGCTTACCCAATCCTTAGCTGGCTAACAATTCCC\r\n",
      "+\r\n",
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\r\n",
      "@read3\r\n",
      "AGTTCTGGCGTCTGAACGTATTATTGTTGCTGGCTATCACAGTTAATTCCCTGCCTACGAATTTTGTCGTACCAA\r\n",
      "+\r\n"
     ]
    }
   ],
   "source": [
    "write_reads(reads_simulation, \"arc_simulation.fastq\")\n",
    "!head -15 arc_simulation.fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0fa432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     387 arc_simulation.fasta\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l arc_simulation.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad944774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[build] loading fasta file arc_simulation.fasta\n",
      "[build] k-mer length: 31\n",
      "[build] counting k-mers ... done.\n",
      "[build] building target de Bruijn graph ...  done \n",
      "[build] creating equivalence classes ...  done\n",
      "[build] target de Bruijn graph has 19 contigs and contains 10000 k-mers \n",
      "\n",
      "\n",
      "[quant] fragment length distribution is truncated gaussian with mean = 150, sd = 20\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 10\n",
      "[index] number of k-mers: 10,000\n",
      "[index] number of equivalence classes: 26\n",
      "[quant] running in single-end mode\n",
      "[quant] will process file 1: arc_simulation.fastq\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 100,000 reads, 99,985 reads pseudoaligned\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 56 rounds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kallisto index -i arc_sim.idx arc_simulation.fasta\n",
    "!kallisto quant -i arc_sim.idx -o arc_sim_output --single -l 150 -s 20 arc_simulation.fastq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80f1b020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_id\tlength\teff_length\test_counts\ttpm\r\n",
      "Arc6\t3000\t2851\t2163.6\t21382.4\r\n",
      "Arc1\t4000\t3851\t2928.33\t21425.1\r\n",
      "Arc9\t3000\t2851\t2804.18\t27713.1\r\n",
      "Arc2\t2000\t1851\t3312.08\t50416.2\r\n",
      "Arc4\t4000\t3851\t10023.1\t73334.2\r\n",
      "Arc7\t2000\t1851\t5382.42\t81930.9\r\n",
      "Arc8\t2000\t1851\t5836.87\t88848.5\r\n",
      "Arc5\t4000\t3851\t12655.6\t92594.4\r\n",
      "Arc10\t3000\t2851\t26131.8\t258255\r\n",
      "Arc3\t3000\t2851\t28747\t284100\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"^#\" arc_sim_output/abundance.tsv\n",
    "!grep -v \"^#\" arc_sim_output/abundance.tsv| sort -n -k5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8282aaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arc1': 5904.1,\n",
       " 'Arc6': 7872.1,\n",
       " 'Arc9': 21648.2,\n",
       " 'Arc2': 57564.6,\n",
       " 'Arc4': 82656.8,\n",
       " 'Arc7': 87084.9,\n",
       " 'Arc8': 88560.9,\n",
       " 'Arc5': 93726.9,\n",
       " 'Arc10': 268634.7,\n",
       " 'Arc3': 286346.9}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_correct_tpm(abundance, lengths):\n",
    "    return {f\"Arc{idx+1}\": round((abund/lengths[idx])/\n",
    "                           sum(np.array(abundance)/np.array(lengths))*\n",
    "                           (10**6), 1)\n",
    "           for idx, abund in enumerate(abundance)}\n",
    "\n",
    "correct_tpm = dict(sorted(calc_correct_tpm(default_abundance, transcript_lengths).items(), \n",
    "                    key=lambda item: item[1]))\n",
    "correct_tpm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6592c9b",
   "metadata": {},
   "source": [
    "It's easy to see that Kallisto calculated some wrong TPM values. Although the ranks of **Arc1**, **Arc6**, and **Arc9** regarding TPM seem to be correspondent to Moriarty's Kallisto result, the excat values are off for quite a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f35c9",
   "metadata": {},
   "source": [
    "## 4. \"debug\" kallisto\n",
    "Maybe you just found that kallisto isn't actually getting its estimation quite right.\n",
    "\n",
    "Suggest a plausible reason that kallisto might be messing up the Arc analysis. What's most unusual about Arc, that might violate kallisto's assumptions somehow? Design at least one experiment that uses your simulator to test your hypothesis -- i.e., identify a modification that gives simulated data that kallisto works fine on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa33b8",
   "metadata": {},
   "source": [
    "### Answer\n",
    "It's quite easy for us to think of the circularity as the reason for the inaccuracy - Too many overlaps shared by each transcript, which might ruin the performance of the underlying algorithm used by Kallisto. We could test this by running the same procedure on a ***\"linear\"*** locus simulation, which is (let's put it simple) longer than the total length of all tanscripts (so no overlaps). Also, let's change the way we mark the start point of each transcript to make there really no overlaps (```seg_start += transcript_len``` rather than ```seg_start += seg_len```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ce71266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_arc_to_transcriptome(arc_locus:str, transcript_lengths:list=transcript_lengths):\n",
    "    \"\"\"\n",
    "    Given an arc lotus, returns a transcriptome\n",
    "    By defult this function uses transcript_lengths provided in the pset\n",
    "    \"\"\"\n",
    "    arc_transcripts = []\n",
    "    seg_start = 0   ## tracking the start position of each transcript\n",
    "    \n",
    "    i=0\n",
    "    for transcript_len in transcript_lengths:\n",
    "        first_seg = arc_locus[seg_start:]  ## there might be a second part in a new circle \n",
    "        if seg_start + transcript_len > len(arc_locus):  ## if completes a circle\n",
    "            arc_transcripts.append(first_seg + arc_locus[:(transcript_len-len(first_seg))])  ## find the second part in the new circle\n",
    "        else:\n",
    "            arc_transcripts.append(arc_locus[seg_start:seg_start+transcript_len])\n",
    "        seg_start += transcript_len\n",
    "\n",
    "\n",
    "    return arc_transcripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb65ccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:14<00:00, 6788.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[build] loading fasta file arc_simulation.fasta\n",
      "[build] k-mer length: 31\n",
      "[build] counting k-mers ... done.\n",
      "[build] building target de Bruijn graph ...  done \n",
      "[build] creating equivalence classes ...  done\n",
      "[build] target de Bruijn graph has 10 contigs and contains 29700 k-mers \n",
      "\n",
      "\n",
      "[quant] fragment length distribution is truncated gaussian with mean = 150, sd = 20\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 10\n",
      "[index] number of k-mers: 29,700\n",
      "[index] number of equivalence classes: 10\n",
      "[quant] running in single-end mode\n",
      "[quant] will process file 1: arc_simulation.fastq\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 100,000 reads, 99,685 reads pseudoaligned\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 52 rounds\n",
      "\n",
      "target_id\tlength\teff_length\test_counts\ttpm\n",
      "Arc1\t4000\t3851\t789\t5738.96\n",
      "Arc6\t3000\t2851\t785\t7712.63\n",
      "Arc9\t3000\t2851\t2095\t20583.4\n",
      "Arc2\t2000\t1851\t3932\t59502.7\n",
      "Arc4\t4000\t3851\t11221\t81618.4\n",
      "Arc7\t2000\t1851\t5921\t89602.2\n",
      "Arc8\t2000\t1851\t6002\t90827.9\n",
      "Arc5\t4000\t3851\t12904\t93860\n",
      "Arc10\t3000\t2851\t27032\t265589\n",
      "Arc3\t3000\t2851\t29004\t284964\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1048)\n",
    "\n",
    "## creat a giant locus so we won't run into overlaps\n",
    "linear_locus = generate_arc_loc(arc_len=999999)\n",
    "\n",
    "transcriptome_simulation = linear_arc_to_transcriptome(linear_locus)\n",
    "write_transcriptome(transcriptome_simulation, \"arc_simulation.fasta\")\n",
    "reads_simulation = transcripts_to_reads(transcripts=transcriptome_simulation)\n",
    "write_reads(reads_simulation, \"arc_simulation.fastq\")\n",
    "!kallisto index -i arc_sim.idx arc_simulation.fasta\n",
    "!kallisto quant -i arc_sim.idx -o arc_sim_output --single -l 150 -s 20 arc_simulation.fastq\n",
    "!grep \"^#\" arc_sim_output/abundance.tsv\n",
    "!grep -v \"^#\" arc_sim_output/abundance.tsv| sort -n -k5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c34854c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arc1': 5904.1,\n",
       " 'Arc6': 7872.1,\n",
       " 'Arc9': 21648.2,\n",
       " 'Arc2': 57564.6,\n",
       " 'Arc4': 82656.8,\n",
       " 'Arc7': 87084.9,\n",
       " 'Arc8': 88560.9,\n",
       " 'Arc5': 93726.9,\n",
       " 'Arc10': 268634.7,\n",
       " 'Arc3': 286346.9}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_tpm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde8c130",
   "metadata": {},
   "source": [
    "The new results look much better and are very similar to the correct TPMs. We could've calculated an error table to compare the before-after results, but the differences are too obvious so I'll just leave it with eyeballing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c150107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
